<!DOCTYPE html>
<html>
  <head>
    
    
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>
      
  Deep Learning Generalization &ndash; Rayncloud.com

    </title>
    
    
    <meta name="description" property="og:description" content="Deep neural networks represent an interesting puzzle when viewed from the perspective of statistical learning theory. Specifically, it is difficult to account for their ability to generalize well.
|Describe what your web page is about">
    

    <meta name="apple-mobile-web-app-title" content="Rayncloud.com">
    
    
    <link rel="icon" href="/favicon-64.png">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <link rel="mask-icon" size="any" href="/pinned-icon.svg">
    
    
    
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@your_twitter_id">
    <meta name="twitter:creator" content="@your_twitter_id">
    <meta name="twitter:title" content="Deep Learning Generalization | Rayncloud.com">
    <meta name="twitter:description" content="Deep neural networks represent an interesting puzzle when viewed from the perspective of statistical learning theory. Specifically, it is difficult to account for their ability to generalize well.|Describe what your web page is about">
    <meta name="twitter:image" content="http://rayncloud.com/twitter-card.png">
    


    <link rel="stylesheet" href="/assets/syntax.css">
    <link rel="stylesheet" href="/assets/primer-build.css">
    <link rel="stylesheet" href="/assets/style.css">
  </head>


  <body class="bg-gray">
    <div id="holy" class="container-lg bg-white h-100">

      <div id="header" class="px-1 bg-white">
        <nav class="UnderlineNav UnderlineNav--right px-2">
  <a class="UnderlineNav-actions muted-link h2" href="http://rayncloud.com/">
    Rayncloud.com
  </a>

  
  
</nav>

      </div>

      <div role="main" id="main" class="holy-main markdown-body px-4 bg-white">
        

<div class="Subhead">
  <div class="Subhead-heading">
    <div class="h1 mt-3 mb-1">Deep Learning Generalization</div>
  </div>
  <div class="Subhead-description">
    


<a href='/categories/machine-learning' class="muted-link">
  <span class="Label Label--gray-darker">Machine Learning</span>
</a>



<a href='/tags/generalization' class="muted-link">
  <span class="Label Label--gray">Generalization</span>
</a>

<a href='/tags/deep-learning' class="muted-link">
  <span class="Label Label--gray">Deep Learning</span>
</a>



    
    <div class="float-md-right">
      <span title="Lastmod: 2019-07-07. Published at: 2019-07-07.">
        
          Lastmod: 2019-07-07
        
      </span>
    </div>
    
  </div>
</div>
<article>
  
  <section class="pb-6 mb-3 border-bottom">
    <p>Deep neural networks represent an interesting puzzle when viewed from the perspective of statistical learning theory. Specifically, it is difficult to account for their ability to generalize well.</p>

<p>Why is this important? Consider the DARPA Learning with Less Labels (LwLL) program, which seeks to achieve robust learning with data sets orders of magnitude smaller than what is currently standard.<sup class="footnote-ref" id="fnref:llwl"><a href="#fn:llwl">1</a></sup> Ultimately, this problem concerns the ability to generalize. Darpa wants to keep the generalization capabilities of deep learning fixed while decreasing the amount of training data. This is tantamount to improving the generalization performance for a fixed amount of training data.</p>

<p>But the generalization capabilities of existing deep learning architectures aren&rsquo;t well understood. Improving generalization will certainly require attaining a better understanding of what DNNs are already doing. The follow-on problem will then be to understand how to use prior knowledge (which may be knowledge of a physical process) to improve upon that existing performance.</p>

<p>Briefly, here is how I understand the problem:</p>

<p>Machine learning algorithms achieve good generalization either through restricting the hypothesis class (e.g., to a set with low VC dimension), or by employing some kind of regularization. Deep neural networks presumably have a very high VC dimension, which implies that something else is helping them to generalize well&mdash;e.g. regularization.</p>

<p>If I had to try to summarize what I think is going on with DNNs, it would be two things:</p>

<ol>
<li><p>The algorithms that exist for training neural networks are performing a sort of implicit regularization&mdash;restricting the set of combinations of parameters able to be learned, and therefore allowing for generalization much better than implied by the VC dimension. Information theory seems to have a lot to say about this regularization, and I&rsquo;m doing my best to obtain the background to understand some of the papers.<sup class="footnote-ref" id="fnref:emergence-of-invariance"><a href="#fn:emergence-of-invariance">2</a></sup>,<sup class="footnote-ref" id="fnref:information-theoretic-deep-learning"><a href="#fn:information-theoretic-deep-learning">3</a></sup></p></li>

<li><p>There is good alignment between the deep neural network architecture and the random physical processes responsible for generating much of the imagery and other data on which deep learning is being used. (I don&rsquo;t understand whether this specifically relates to generalization; I would guess that it more explains why deep neural networks achieve good approximation error, i.e., can achieve a very good fit to training data).[^deep-cheap-learning]</p></li>
</ol>

<p>So, how do we improve the generalization capabilities of deep neural networks? I think it requires having a robust understanding of the two points above. Perhaps better performance will require stronger, explicit regularization (#1)&ndash;but this must not interfere with #2.  Essentially, if we understand the nature of both #1 and #2, then perhaps we can couple that with knowledge of the problem to design regularization that clamps down the generalization error with a smaller training set.</p>

<p>Image reconstruction problems (regression), as opposed to classification problems, are perhaps one of the best sandboxes for exploring these questions, because the degradation process that the DNN is trying to invert can be explicitly modeled much more easily than the generative process being inverted in classification problems. So there is plenty of &ldquo;knowledge&rdquo;&mdash;the issue is how to incorporate this knowledge into the learner. A good first step in this area may trying to understand the relationship between DNNs and Bayesian techniques/energy minimization techniques that have become common in the context of image reconstruction. These techniques often make explicit use of knowledge of the process by which the data was generated. There is some work connecting Deep Learning to energy minimization techniques, but it is difficult to decipher without a strong background in functional analysis.<sup class="footnote-ref" id="fnref:cnn-inverse-problems"><a href="#fn:cnn-inverse-problems">4</a></sup></p>
<div class="footnotes">

<hr />

<ol>
<li id="fn:llwl"><a href="https://www.darpa.mil/news-events/2018-07-11">Darpa News</a>, <a href="https://www.darpa.mil/program/learning-with-less-labels">Program</a>, <a href="https://www.northeastern.edu/resdev/funding-announcement/learning-with-less-labels-lwll/">Technical Compontents</a>
 <a class="footnote-return" href="#fnref:llwl"><sup>[return]</sup></a></li>
<li id="fn:emergence-of-invariance">A. Achille and S. Soatto, “<a href="https://arxiv.org/abs/1706.01350">Emergence of Invariance and Disentanglement in Deep Representations</a>,” arXiv:1706.01350 [cs, stat], Jun. 2017.
 <a class="footnote-return" href="#fnref:emergence-of-invariance"><sup>[return]</sup></a></li>
<li id="fn:information-theoretic-deep-learning">J. Zhang, T. Liu, and D. Tao, “<a href="https://arxiv.org/abs/1804.09060">An Information-Theoretic View for Deep Learning</a>,” arXiv:1804.09060 [cs, stat], Apr. 2018.
 <a class="footnote-return" href="#fnref:information-theoretic-deep-learning"><sup>[return]</sup></a></li>
<li id="fn:cnn-inverse-problems">K. H. Jin, M. T. McCann, E. Froustey, and M. Unser, “<a href="https://arxiv.org/pdf/1611.03679">Deep Convolutional Neural Network for Inverse Problems in Imaging</a>,” IEEE Trans. on Image Process., vol. 26, no. 9, pp. 4509–4522, Sep. 2017.
 <a class="footnote-return" href="#fnref:cnn-inverse-problems"><sup>[return]</sup></a></li>
</ol>
</div>
  </section>

  <section>
    
      
    
  </section>
</article>

      </div>

      <div id="side" class="pr-1 bg-white">
        <aside class="pr-3">
          
  
    <div id="toc" class="Box Box--blue mb-3">
      <b>Deep Learning Generalization</b>
      
    </div>
  

  
    
    
      <div>
        
          <a href="https://twitter.com/share?ref_src=twsrc%5Etfw" class="twitter-share-button" data-show-count="false">Tweet</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
        

        
          <iframe src="https://www.facebook.com/plugins/share_button.php?href=https%3A%2F%2Fdevelopers.facebook.com%2Fdocs%2Fplugins%2F&layout=button&size=small&mobile_iframe=true&width=61&height=20&appId" width="61" height="20" style="border:none;overflow:hidden" scrolling="no" frameborder="0" allowTransparency="true" allow="encrypted-media"></iframe>
        

        

        
          <a data-pocket-label="pocket" data-pocket-count="none" class="pocket-btn" data-lang="en"></a>
          <script type="text/javascript">!function(d,i){if(!d.getElementById(i)){var j=d.createElement("script");j.id=i;j.src="https://widgets.getpocket.com/v1/j/btn.js?v=1";var w=d.getElementById(i);d.body.appendChild(j);}}(document,"pocket-btn-js");</script>
        

      </div>
    
  

        </aside>
      </div>

      <div id="footer" class="pt-2 pb-3 bg-white text-center">
        

  <span class="text-small text-gray">
    

    Powered by 
    <a href="https://gohugo.io" class="link-gray-dark">Hugo</a>.
  </span>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: { equationNumbers: { autoNumber: "AMS" } }
    });
</script>
      </div>
    </div>


    
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/x-mathjax-config">MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });</script>
  </body>
</html>
