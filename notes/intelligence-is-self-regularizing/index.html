<!DOCTYPE html>
<html>
  <head>
    
    
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>
      
  Alignment via Intelligence &ndash; Rayncloud.com

    </title>
    
    
    <meta name="description" property="og:description" content=" An initial reaction to AI alignment concerns. |Describe what your web page is about">
    

    <meta name="apple-mobile-web-app-title" content="Rayncloud.com">
    
    
    <link rel="icon" href="/favicon-64.png">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <link rel="mask-icon" size="any" href="/pinned-icon.svg">
    
    
    
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@your_twitter_id">
    <meta name="twitter:creator" content="@your_twitter_id">
    <meta name="twitter:title" content="Alignment via Intelligence | Rayncloud.com">
    <meta name="twitter:description" content="
An initial reaction to AI alignment concerns. 
|Describe what your web page is about">
    <meta name="twitter:image" content="http://rayncloud.com/twitter-card.png">
    


    <link rel="stylesheet" href="/assets/syntax.css">
    <link rel="stylesheet" href="/assets/primer-build.css">
    <link rel="stylesheet" href="/assets/style.css">
    <link rel="stylesheet" href="/assets/custom.css">
    <script defer src="/assets/scripts.js"></script>
  </head>


  <body class="single">
    <div id="holy" class="container-md bg-white h-100">

      <div id="header" class="px-1 bg-white">
        <nav class="UnderlineNav UnderlineNav--right px-2">
  <a class="UnderlineNav-actions muted-link h2" href="http://rayncloud.com/">
    Rayncloud.com
  </a>

  
  
</nav>

      </div>

      <div role="main" id="main" class="holy-main markdown-body px-4 bg-white mt-5">
        

<div class="Subhead">
  <div class="Subhead-heading">
    <div class="h1 mt-3 mb-1">Alignment via Intelligence</div>
  </div>
  <div class="Subhead-description">
    


<a href='/categories/alignment' class="muted-link">
  <span class="Label Label--gray-darker">Alignment</span>
</a>





    
    <div class="float-md-right">
      <span title="Lastmod: 2023-03-10. Published at: 2023-03-05.">
        
          Lastmod: 2023-03-10
        
      </span>
    </div>
    
  </div>
</div>
<article>
  
  <section class="pb-6 mb-3 border-bottom">
    <div class="summary">
An initial reaction to AI alignment concerns. 
</div>
<p>I recently listened to Eliezer Yudkowsky&rsquo;s <a href="https://www.youtube.com/watch?v=gA1sNLL6yg4">Bankless interview</a>, which has been making the rounds on social media. This post contains my initial reaction as someone who has 1) spent a lot of time thinking about statistical learning theory and how it relates to capabilities of deep neural networks, but 2) not lots of time (except maybe the brief journal note here and there) thinking about AI alignment.</p>
<p>With the attention generated by models such as ChatGPT and Sydney (Bing), a huge number of people has suddenly become attuned to the fact that AI is about to massively change the world. Frankly, it looks very dangerous. And as people wonder about what dangers they should be worrying abot, they will inevitably come across very intelligent people who have been thinking about this issue for a very long time, and who say that the danger (or even the foregone conclusion in Eliezer&rsquo;s case) is that an unaligned AI will destroy the world. And people here this is think &ldquo;Ah, yes, I knew I was worried about something dire&hellip;&rdquo;</p>
<p>So we&rsquo;ve suddenly found ourselves in a strange cultural moment where it appears obvious to many people that we will give birth to a superintelligent AI that will kill us all.</p>
<p>For my part, I&rsquo;ve never found this conclusion at all obvious. And with these worries entering the mainstream, I recently made plans to spend some of my free time carefully reviewing the arguments of people like Eliezer.</p>
<p>However, after I listened to the Bankless podcast, I found that I had quite a few thoughts in response. So I thought it would be worth taking a bit of time to formulate these thoughts before allowing the ideas of others to reframe my way of thinking about the issue. That&rsquo;s what this post is. I&rsquo;m interested to see, after &ldquo;reading the arguments&rdquo;, whether my initial objections will be quite squelched, or whether I&rsquo;ll find myself wanting to continue exploring some of the lines of thought explored within this post.</p>
<h3 id="some-basic-alignment-problems">Some basic alignment problems</h3>
<p>As I listend to the the bankless podcast I was struck by the notion that, although superintelligence is at the root of the overall alignment worry, it also seems like it could be the seed of the solution. In fact, I might even go so far as to put it this way: The problem that Eliezer is worried about is not superintelligence but super<em>optimization.</em> And the solution to this problem is, in a word, intelligence.</p>
<p>Among Eliezer&rsquo;s various worries about AI, which I assume to be many, it seemed to me that two in particular came to forefront within the podcast:</p>
<ol>
<li>Suppose we were to specify a utility function, and then apply some super powerful optimizer toward maximizing this utility function. The problem is that we don&rsquo;t know how to specify a utility function such that&ndash;when optimized strongly enough&ndash;it won&rsquo;t be a result we didn&rsquo;t want; one that clashed with our basic values.</li>
<li>The second problem is that we might optimize an agent to solve a particular utility function in the context of one input distribution, but find that the policies learned by the agent are arbitrarily poorly aligned with the original utility function when the distribution shifts somehow.</li>
</ol>
<p>In this post, I&rsquo;ll explore the idea that intelligence is in a way &ldquo;self-regularizing&rdquo; in the context of each of these problems.</p>
<p>True to my overall reframing, I&rsquo;ve posed these problems in terms of optimization. Intelligence might come into play here in different ways; for instance, our super-powerful optimizer in problem 1 could be a superintelligent agent. Or, intelligence could be an emergent property of the agent optimized in problem 2. While I don&rsquo;t think that intelligence deserves a place in the problem statement, per se, I do think that it is hard to imagine the types of world ending scenarios that Eliezer paints without intelligence coming into the picture. We&rsquo;re usually envisioning an AI that ends the world right underneath our noses without use noticing or being able to stop it.</p>
<p>I&rsquo;ll dwell on this point a little further. The important quality of the optimizer in problem 1 is its raw optimization power, and pointedly not the set of interfaces or actuators that it has access to. This is an agent that can destroy the world by displaying text to manipulate a human into sending an email that ends the world. So in my treatment of problem 1, I will stick to a highly constrained agent of arbitrary power.</p>
<p>One thing to notice is that solving problem 1 is dependent on solving problem 2. You could roughly decompose the two problems in the following way: For an agent to take an action aligned with the intent of my instruction it must both a) understand my intent and how to enact it and b) <em>want</em> to comply with my instructions. I&rsquo;m using the word <em>want</em> here to match Eliezer&rsquo;s vocabulary. The word isn&rsquo;t important. As long as condition a) is well-defined and satisfied, condition b) simply regards any tendency for there to be a residual between what the agent did and what it understood that the user wanted it to do. In our discussion of problem 1, we&rsquo;ll assume that problem 2 has already been solved.</p>
<p>I&rsquo;ll spend the remainder of the post describing the hopefully thought-provoking ways in which I think intelligence is actually part of the solution to each of these problems.</p>
<h3 id="1-the-problem-of-utility-function-misspecification">1. The problem of utility function misspecification</h3>
<p>The cute way of saying my point concerning problem 1 is that any agent smart enough to end the world ala Eliezer&rsquo;s nightmare must also be smart enough to understand that we don&rsquo;t want it to do this. Cuteness aside, what I actually mean to do here is to propose that there are ways of leveraging the intelligence of an agent to solve the alignment problem.</p>
<p>It is perfectly possible to tell a human (with human-level intelligence), to observe certain constraints while solving a problem. A human may be able to find loopholes within your constraints which allow them to improve the optimization performance. But a human is also intelligent enough to infer an intent behind your constraints and recognize when an optimization is exploiting a loophole. It is quite easy to convey to a human-level intelligence the concept that I want the solution to satisfy the general intent of a set of constraints more so than the exact letter of the constraints.</p>
<p>This capability only improves with intelligence. A sufficiently intelligent agent could have a general model of human beings that could be fine-tuned to particular instances. Constraints of the optimization could then be defined on the basis of this model. You can tell your friend &ldquo;don&rsquo;t do anything I wouldn&rsquo;t do&rdquo; or to consider &ldquo;what would Jesus do?&rdquo; You can ask the AI not to do anything that a committee of leading ethics wouldn&rsquo;t do (though this might actually be the worst type of committee to choose&hellip;).</p>
<p>I could easily be wrong, but my hunch is that many people in the alignment space are trying to &ldquo;hand code&rdquo; a utility function that will force an AI to toe the line&ndash;improving on Isaac Asimov&rsquo;s three laws. Just like hand-coding an ImageNet classifier, this is an extremely difficult problem. It&rsquo;s very analogous to what OpenAI is trying to do with RLHF training with ChatGPT and Bing&rsquo;s Sydney.</p>
<p>Now, whether it&rsquo;s possible to do something like what I am saying with an AI agent depends of course on the base objective (utility function) on which it has been optimized/trained. I think that as far as this possibility is concerned, recent developments such as ChatGPT are very suggestive.</p>
<p>Let us take the objective of the GPT series: next-token prediction for internet text. We&rsquo;ll ignore practicalities such as training methodology, and simply imagine the agent which maximizes this objective (Implicit here is that we are maximizing performance on a test set which we randomly generate from distribution each time we use it. We&rsquo;ll deal with statistical issues in the next section) (This maximizer might not be defined if we don&rsquo;t constrain it somehow, so let&rsquo;s just say it&rsquo;s constrained only by the amount of compute, memory, and time needed to evaluate the model. In particular, it&rsquo;s not constrained by any limitations of training. It simply <em>is</em> the best solution subject to these physical constraints).</p>
<p>This agent is a superintelligence by almost any standard. Let&rsquo;s look at some examples of problems which it can solve:</p>
<ul>
<li>I can describe a math problem which human mathematicians puzzled over for centuries, and it will give me the solution after some amount of time.</li>
<li>I can describe the preamble to a 5-year analysis by the world&rsquo;s top climate scientists of some data which I have supplied. After some amount of time, it will give me the rest of the report.</li>
</ul>
<p>Naturally, these don&rsquo;t need to be actual problems which were actually solved or reports which were actually written.</p>
<p>There&rsquo;s almost nothing more to be said here. By simply appropriately prompting the model, the values of human society are already accounted for. We&rsquo;re done. We could provide explicit instructions or constraints if we needed to; it almost makes no difference.</p>
<p>Of course there are various ways that this approach could go a bit wrong. The training data might contain examples of reports which were fallacious or human actors who were deceitful. The AI model will be capable of producing such outputs. But there appear to be plenty of ways to prompt our way out of such possibilities, or make use of the model to detect them if they were to occur.</p>
<p>In closing out this section, let me just reiterate that I&rsquo;m not suggesting that this is actually the most effective/practical way to build an AI, and I&rsquo;m not interested here in the second order reasons why a super-GPT might not be what humanity opts to build. I&rsquo;m simply responding to the claim that we don&rsquo;t know of any ways to solve problem 1.</p>
<p>Clearly, my answer to problem 1 is quite empty without an answer to problem 2 at hand. I think that probably the more interesting discussion involves how intelligence can address the subtle problems of distribution-shift and the emergence of proto-wants.</p>
<h3 id="2-the-problem-of-proto-wants--distribution-shift">2. The problem of proto-wants / distribution shift</h3>
<p>The problem of distribution shift is quite familiar in machine learning research. Imagine that you perform some data driven optimization in order to teach a self-driving car to navigate a city based on inputs from its array of sensors. But all of your training data comes from bright, sunny days in the month of July. How will your optimized set of policies for self-driving perform on a rainy day in November?</p>
<p>The answer is that that they will probably fail very badly and result in a possibly disasterous outcome.</p>
<p>Now, a basic question is whether intelligence is helpful or harmful in this situation. It&rsquo;s actually pretty obvious that, at least up unto a point, intelligence is <em>exactly</em> what we need to solve the distributional shift problem.</p>
<p>It&rsquo;s helpful here to look at Eliezer&rsquo;s own example of evolutionary optimization for genetic fitness in humans. He&rsquo;s worried things like ice cream.</p>
<p>Million&rsquo;s of years of evolution optimized humans for survival within an environment that was hugely different than the one in which the majority of modern humans find themselves. In that environment, it was a good strategy for humans to consume as much of sweet, salty, and fatty goods as we could get into our mouths. And now this means that we are very liable to overindulge in ice cream, even when this is counterproductive to our genetic fitness.</p>
<p>This is an example of an instance in which our internal <em>wants</em> (eating lots of food) are not precisely aligned with the original objective of the optimization (genetic fitness). Somehow, evolution didn&rsquo;t manage to make us actually instinctually want nothing more than the replication of our genes. This is an important point, and we will return to it.</p>
<p>But before doing so, let&rsquo;s take a step back and consider two broader points:</p>
<ol>
<li>
<p>Evolution imbued us with a host of wants and instincts which&ndash;in the context of humanity&rsquo;s great environmental shift&ndash;no longer serve their original function. Ironically, intelligence is the marked exception. Intelligence originated because it helped us to navigate, understand, and manipulate our world. And it <em>still</em> allows us to navigate, understand, and manipulate the world, even though our environment has changed drastically.</p>
</li>
<li>
<p>Up unto a point, intelligence clearly makes us more robust to the problem of distributional shift. Again, take ice cream. You love ice cream. You <em>want</em> ice cream. But you also have some self-awareness about this fact. Eating ice cream isn&rsquo;t just something that you automatically, compulsively do when ice cream is presented to you. You become aware of the particular desire, you evaluate it against other objectives that you may have (which on the whole are probably more aligned with the original optimization objective), and ultimately make some kind of decision.</p>
</li>
</ol>
<p>The problem really is the &ldquo;up to a point&rdquo; in 2.</p>
<p>Let&rsquo;s first reiterate the model in question. We&rsquo;ll think of the intelligent agent having it&rsquo;s own inner objective which could be different than our imposed outer optimization objective. When these are identical, all is well and intelligence assumes its role up mitigating the problem of distribution shift. After all, an intelligent agent not only has a good strategy, but also an understanding of when and why that strategy works, and an ability to find new strategies when conditions change. But if the innter and outer objectives differ, we&rsquo;d expect the intelligence to optimize our outer objective under the training distribution, but no longer to do so in the presence of certain distribution shifts. It could even work at cross-purposes.</p>
<p>The worry is that we end up having a situation like modern humans and birth control. Ultimately, the intelligence, acting on it&rsquo;s internal objectives (sex and ice-cream) completely circumvents the original objective (children).</p>
<p>So how do we solve this problem of alignment between inner and outer objectives?</p>
<h4 id="the-case-for-optimism">The case for optimism</h4>
<p>Let me first paint the case for optimisim.</p>
<p>In many ways, the inner alignmnet problem is akin to other problems which the grand program of graduate student descent in machine learning research is already solving. I&rsquo;ll back up a bit in order to make this perspective clear.</p>
<p>Let me start by clarifying a bit what I mean by intelligence. In recent years, with Deep Learning&ndash;an instance of a &ldquo;Machine Learning&rdquo; technology&ndash;being the premiere instance of the category of technologies known as &ldquo;Artificial Intelligence,&rdquo; it is probably easy to assume that intelligence and capacity for learning are somehow related or even identical. However, it&rsquo;s probably better to think of intelligence as a set of qualitative capabilities that may emerge from optimizing to an objective. Here, this is Optimization with a capital O, at a societal level. Not just the result of, say, performing a numerical optimization with something like SGD, but also the result of architectural optimization resulting from trial and error and even principled reasoning based on the results of &ldquo;knowledge&rdquo; of our universe based on millennia of cultural learning / science.</p>
<p>How we will arrive at an intelligent model is likely in the same way that we have arrived at deep learning models which dazzle with their current generalization capabilities: By setting appropriate learning objectives and then performing graduate student descent over the space of architectures, hyperparameters, training methodologies, etc. Eventually, the learning objective for which the great corpus of AI labs and graduate students is optimizing will be expansive enough in its requirements for robustness to things like distribution shifts and adversarial examples that it will require something like human intelligence to meet the objective well.</p>
<p>So now we have an intelligent model, but one that may have some amount of inner/outer misalignment. But since this misalignment is again a kind of lack of robustness, we can again suppress it by continuing to make the objective to which we optimize even more expansive/demanding. In this sense, solving this aspect of the alignment problem is &ldquo;well aligned&rdquo; with what researchers and the field in general are likely to be already doing!</p>
<h4 id="the-worry">The worry</h4>
<p>The biggest worry is that the problem of inner alignment via grad student descent turns out to be a hard problem in some strong sense of the word hard. Perhaps this hardness is amplified by the existence of the intelligence within the picture. Or alternatively, the inner alignment problem could simply be significantly harder than the problem of creating a powerful superintelligence. In either case, it appears that we could end up in a scenario where inner/outer misalignment results in the intelligence subverting the original objective.</p>
<h3 id="closing-thoughts">Closing Thoughts</h3>
<p>A confession: While writing this post, I skimmed over the paper which discusses mesa-optimization, which appears to be related to the worry of the final section. On my first encounter with the mesa-optimization worry, it seemed highly exotic and contrived. Only after a couple of drafts and refactorings of this post did Eliezer&rsquo;s point finally become clear.</p>
<p>Much seems to hinge on the difficulty of achieving inter-alignment and whether this problem is difficult in some kind of adversarial sense or only difficult in an engineering sense (like problems that are already being routinely solved in ML). I&rsquo;m pretty interested in diving into the work that has been done on this topic to see how people are modeling the problem.</p>
  </section>

  <section>
    
      
    
  </section>
</article>

      </div>


      <div id="footer" class="pt-2 pb-3 bg-white text-center">
        

  <span class="text-small text-gray">
    

    Powered by 
    <a href="https://gohugo.io" class="link-gray-dark">Hugo</a>.
  </span>



      </div>
    </div>

  </body>
</html>
