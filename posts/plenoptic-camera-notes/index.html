<!DOCTYPE html>
<html>
  <head>
    
    
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>
      
  Plenoptic Camera Notes &ndash; Technical Notes

    </title>
    
    
    <meta name="description" property="og:description" content="Working notes related to the use of a plenoptic camera for estimating depth within a scene.
|Describe what your web page is about">
    

    <meta name="apple-mobile-web-app-title" content="Technical Notes">
    
    
    <link rel="icon" href="/favicon-64.png">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <link rel="mask-icon" size="any" href="/pinned-icon.svg">
    
    
    
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@your_twitter_id">
    <meta name="twitter:creator" content="@your_twitter_id">
    <meta name="twitter:title" content="Plenoptic Camera Notes | Technical Notes">
    <meta name="twitter:description" content="Working notes related to the use of a plenoptic camera for estimating depth within a scene.|Describe what your web page is about">
    <meta name="twitter:image" content="http://rayncloud.com/twitter-card.png">
    


    <link rel="stylesheet" href="/assets/syntax.css">
    <link rel="stylesheet" href="/assets/primer-build.css">
    <link rel="stylesheet" href="/assets/style.css">
  </head>


  <body class="bg-gray">
    <div id="holy" class="container-lg bg-white h-100">

      <div id="header" class="px-1 bg-white">
        <nav class="UnderlineNav UnderlineNav--right px-2">
  <a class="UnderlineNav-actions muted-link h2" href="http://rayncloud.com/">
    Technical Notes
  </a>

  
  
</nav>

      </div>

      <div role="main" id="main" class="holy-main markdown-body px-4 bg-white">
        

<div class="Subhead">
  <div class="Subhead-heading">
    <div class="h1 mt-3 mb-1">Plenoptic Camera Notes</div>
  </div>
  <div class="Subhead-description">
    






    
    <div class="float-md-right">
      <span title="Lastmod: 2019-07-03. Published at: 2019-07-01.">
        
          Lastmod: 2019-07-03
        
      </span>
    </div>
    
  </div>
</div>
<article>
  
  <section class="pb-6 mb-3 border-bottom">
    <p>Working notes related to the use of a plenoptic camera for estimating depth within a scene.</p>

<h1 id="outline">Outline</h1>

<p>This working paper has two aims related to the implementation of plenoptic cameras as a situational-awareness diagnostic for autonomous vehicles. The first aim is to understand trade-offs in the plenoptic camera&rsquo;s light field sampling. From a classical estimation perspective, we first wish to characterize the trade-off between the angular sampling of the camera and the lower bound on depth uncertainty. Secondly, from a Bayesian perspective, we wish to characterize the expected value of this uncertainty for typical scenes. Finally, we wish to answer the question of whether the appropriate use of light field priors will impact the optimal trade-off between angular and spatial sampling. The second aim is to develop an optimal algorithm for estimating depth from the sampled light field. There are many possible approaches to this problem. We plan to start by assessing the practicality of MLE and MAP estimators. Provided that such estimators are workable, we will compare them with more heuristic approaches found within the literature. Finally, approaches based on compressive sensing and machine learning will be addressed.</p>

<h1 id="plenoptic-camera-trade-offs">Plenoptic Camera Trade-offs</h1>

<p>The objective of this section is to determine the light field measurement architecture with ideal properties for the application of infrared light field imaging to autonomous vehicles.</p>

<h2 id="background">Background</h2>

<p>The plenoptic camera samples the 4D light field function, which contains features correlating to the depth of objects within a scene. However, there also exist other types of computational imagery architectures which measure different linear projections of the 4D light field. Coupled with modern statistical image / light field priors, these architectures can also be used to estimate the original light field as a statistical inference problem. A logically preliminary goal would be to assess which architectures&rsquo; projections optimally make use of light field priors, as in Levin<sup class="footnote-ref" id="fnref:1"><a href="#fn:1">1</a></sup>. However, we confine ourselves to looking only at the trade-offs contained in the plenoptic camera architecture.</p>

<h2 id="light-field-sampling">Light Field Sampling</h2>

<p>The plenoptic camera samples the 4D radiance function at a single plane within camera, commonly referred to as a light field. Commonly, this radiance function is parameterized by the points $(s,t)$ in the image plane ($I$) and $(u,v)$ in the pupil plane ($P$) such that</p>

<p>\begin{equation}
  L_I(s,t,u,v)\frac{dsdtdudv}{l^2}
\end{equation}</p>

<p>gives the radiant flux along the ray from the region $dsdt$ to the region $dudv$.</p>

<p>A more appropriate parameterization for the task of estimating depth makes use of the point $(u,v)$ on the pupil plane and the point $(S,T)$ on the conjugate of the image plane $&copy;$, located a distance $L$ away from the pupil, where</p>

<p>\begin{equation}
  \frac{1}{L}+\frac{1}{l}=\frac{1}{f}
\end{equation}</p>

<p>By conservation of energy, $L_I(s,t,u,v) = L_C(s/M,t/M,u,v)$ where $M = L/l$.</p>

<p>Next, we consider how the radiance from a point $(x,y,z)$ is represented within $L_O$. Formally, given a point at $(x,y,z)$ on a Lambertian surface with radiance $L_O$, what is the set $(S,T,u,v)$ such that $L_C(S,T,u,v) = L_O$? The figure shows that this set will contain all elements satisfying the equation</p>

<p>\begin{equation}
    \frac{u-x}{z} = \frac{x-S}{L-z},
\end{equation}</p>

<p>or</p>

<p>\begin{equation}
    \label{eq:objectslope}
    S = x(1-\alpha) + u\alpha,
\end{equation}</p>

<p>where $\alpha = 1-L/z$. Moreover, because $S = s/M$, it can be shown that</p>

<p>\begin{equation}
    s = x_i(1-\beta)+u\beta = s_0 + u\beta,
\end{equation}</p>

<p>where $\beta = M\alpha = 1 - l/z_i$ and $s_0$ is the center of the circle of confusion from the point on the $I$ plane. To summarize,</p>

<p>\begin{equation}
    \label{eq:lfrelationships}
    L_I(s_o+u\beta,t_0+v\beta,u,v) = L_C(x(1-\alpha) + u\alpha,y(1-\alpha) + v\alpha,u,v) = L_O(x,y,z).
\end{equation}</p>

<p>The plenoptic camera samples the light field along the $s$, $t$, $u$, and $v$ dimensions, subject to the constraint that the product of the number of samples along each dimension is equal to the total number of detectors, considered to be fixed for the purposes of all analysis that follows, i.e.,</p>

<p>\begin{equation}
    N = N_sN_tN_uN_v.
\end{equation}</p>

<p>We assume additive white Gaussian noise, so that the signal can be modeled as</p>

<p>\begin{equation}
    \label{eq:model}
    \Phi_{ijkl} = \mu_{ijkl} + w_{ijkl},
\end{equation}</p>

<p>where the expected value $\mu_{ijkl}$ is integrated radiance over the 4D sample size</p>

<p>\begin{equation}
    \label{eq:lfexpectedvalue}
    \mu_{ijkl} = \frac{1}{l^2}\int\int\int\int{}L_I(s,t,u,v)dsdtdudv.
\end{equation}</p>

<h2 id="classical-estimation-paradigm">Classical Estimation Paradigm</h2>

<p>To begin our depth uncertainty analysis, we will employ a classical estimation framework to lower bound the depth estimation variance for an unbiased estimator. For this to be feasible, we will need to assume a simple model for the scene. We will begin with a model consisting of a simple edge of uniform depth and known magnitude. Given this model, we can easily determine how changing the depth of the edge affects the information contained in the sampled light field, allowing for the determination of the Fisher information and Cramer-Rao Lower Bound (CRLB). Outside of the simple edge model, it will be difficult to select object-space models leading to tractable determinations of the CRLB. Instead, we will move toward modeling the sampled light field itself, and looking at the CRLB for the light field slope estimation. Even given this simplification, it will not be possible to estimate the uncertainty for a complex scene due to the confounding effects of neighboring pixels. At this point, we will move to a Bayesian estimation framework, using generated light fields to implicitly model the light field prior distribution.</p>

<h3 id="simple-edge-model">Simple Edge Model</h3>

<p>Given (\ref{eq:model}), the probability of a given light field observation $\mathbf{\Phi}$ is given by</p>

<p>\begin{equation}
    p(\mathbf{\Phi};z) = \frac{1}{(2\pi^2\sigma^2)^{N/2}}\mathrm{exp}\left(-\frac{1}{2\sigma^2}\sum_{ijkl}(\Phi_{ijkl}-\mu_{ijkl})^2\right).
\end{equation}</p>

<p>We wish to determine the Fisher information, $I(z)$, which is given by</p>

<p>\begin{equation}
    \label{eq:fisherinfo}
    I(z) = -E\left(\frac{\partial^2\mathrm{ln}p(\mathbf{\Phi};z)}{\partial{z}^2}\right) = \frac{1}{\sigma^2}\sum_{ijkl}\left(\frac{\partial\mu_{ijkl}}{\partial{z}}\right)^2.
\end{equation}</p>

<p>We will then have that the variance of any estimator $\hat{z}$ is bounded by the reciprocal of $I(z)$, i.e.,</p>

<p>\begin{equation}
    \mathrm{var}(\hat{z}) \geq \frac{1}{I(z)}
\end{equation}</p>

<p>In order to evaluate the derivative in (\ref{eq:fisherinfo}), one approach is to recast (\ref{eq:lfexpectedvalue}), as in</p>

<p>\begin{equation}
    \label{eq:objectmodel}
    \mu_{ijkl} = \frac{1}{z^2}\int_{u_k}^{u_{k+1}}\int_{v_l}^{v_{l+1}}\int_{x_i(u,z)}^{x_{i+1}(u,z)}\int_{y_i(v,z)}^{y_{j+1}(v,z)}L_O(x,y,z)dxdy,
\end{equation}</p>

<p>such that it explicitly shows the sampling of the radiance from an object located at a fixed depth, $z$. However, taking the derivative of (\ref{eq:objectmodel}) reveals a number of competing radiometric effects, namely the decreasing solid angle of the pupil vs the increasing field of view of the detector. Demonstrating that these effects cancel out is cumbersome. It is much easier to use (\ref{eq:lfexpectedvalue}) directly, making use of the equivalence of radiance indicated in (\ref{eq:lfrelationships}). To evaluate the integral, we define the edge model,</p>

<div>\begin{equation}
    L_O(x,y,z) = 
    \begin{cases} 
      L_0 & x < 0 \\
      0 & o.w.
   \end{cases}.
\end{equation}</div>

<p>We can then use (\ref{eq:objectslope}) to show that $L_I$ is equal to</p>

<div>\begin{equation}
    L_I(s,t,u,v) = 
    \begin{cases} 
      L_0 & s < u\left(\frac{l}{L}-\frac{l}{z}\right) \\
      0 & o.w.
   \end{cases}.
\end{equation}</div>

<p>Note that for samples which do not contain the edge, the expected value of $\mu_{ijkl}$ does not change with $z$. For the $N_tN_v$ samples which contain the edge, the expected value simplifies to</p>

<p>\begin{equation}
    \mu_{ijkl} = \frac{\Delta{t}\Delta{v}}{l^2}\int_{u_k}^{u_{k+1}}\int_{s_i}^{u\left(\frac{l}{L}-\frac{l}{z}\right)}L_0dsdu = \frac{L_0\Delta{t}\Delta{v}}{l^2}\int_{u_k}^{u_{k+1}}\left[u\left(\frac{l}{L}-\frac{l}{z}\right)-s_i\right]du
\end{equation}</p>

<p>Differentiating with respect to $z$ gives</p>

<div>\begin{equation}
    \frac{\partial\mu_{ijkl}}{\partial{z}} =\frac{L_0\Delta{t}\Delta{v}}{l^2}\int_{u_k}^{u_{k+1}}\frac{ul}{z^2}du = \frac{L_0\Delta{t}\Delta{v}\Delta{u}^2}{lz^2}(k+1/2)
\end{equation}</div>

<p>This can be rewritten as follows,</p>

<p>\begin{equation}
    \frac{\partial\mu_{ijkl}}{\partial{z}} = \frac{L_0\Delta{t}\Delta{s}\Delta{u}\Delta{v}}{l^2}\frac{l}{z^2}\frac{\Delta{u}}{\Delta{s}}k = L_0\Delta{q}^2(FN)^2\frac{l}{z^2}\frac{\Delta{u}}{\Delta{s}}k,
\end{equation}</p>

<p>where the factor $\Delta{L}\Delta{q}^2(FN)^2$ can be considered the average signal detected due to the step $\Delta{L}$ given a conventional camera with detector size $\Delta{q}$.
We can now solve for the fisher information via (\ref{eq:fisherinfo}),</p>

<div>\begin{multline}
    I_{total}(z) = \left(\frac{L_0\Delta{q}^2(FN)^2}{\sigma}\frac{l}{z^2}\frac{\Delta{u}}{\Delta{s}}\right)^2N_tN_v\sum_k^{N_u}(k+1/2)^2 \\
     = \left(SNR\frac{l}{z^2}\frac{\Delta{u}}{\Delta{s}}\right)^2N_tN_v\left(\frac{N_u^3}{3}+N_u^2+\frac{11N_u}{12}\right),
\end{multline}</div>

<p>where we have interpretted the first factor in parenthesis as the SNR resulting from the edge discrepancy in a conventional camera. This is the information about the depth of the entire surface as a whole. Since this information was derived from the information in the $N_t$ pixels containing the edge, we would do better to divide the information up among these pixels giving $I(z) = I_{total}(z)/N_t$ if we wish to come closer to the understanding the performance of independent point estimation.</p>

<p>The final step is to invert the Fisher information and substitute for $\Delta{u}/\Delta{s}$ to obtain the lower bound on variance:</p>

<p>\begin{equation}
    \mathrm{var}(\hat{z}) \geq \frac{N_u/N_v}{\frac{1}{3}+\frac{1}{N_u}+\frac{11}{12}\frac{1}{N_u^2}}\left(\frac{z^2\Delta{q}/Dl}{\mathrm{SNR}}\right)^2
\end{equation}</p>

<p>Interestingly, for the edge model, the achievable estimation performance is not improved by increasing angular samples.</p>
<div class="footnotes">

<hr />

<ol>
<li id="fn:1">Levin, A., Freeman, W. T., &amp; Durand, F. (2008, October). Understanding camera trade-offs through a Bayesian analysis of light field projections. In European Conference on Computer Vision (pp. 88-101). Springer, Berlin, Heidelberg.
 <a class="footnote-return" href="#fnref:1"><sup>[return]</sup></a></li>
</ol>
</div>
  </section>

  <section>
    
      
    
  </section>
</article>

      </div>

      <div id="side" class="pr-1 bg-white">
        <aside class="pr-3">
          
  
    <div id="toc" class="Box Box--blue mb-3">
      <b>Plenoptic Camera Notes</b>
      <nav id="TableOfContents">
<ul>
<li><a href="#outline">Outline</a></li>
<li><a href="#plenoptic-camera-trade-offs">Plenoptic Camera Trade-offs</a>
<ul>
<li><a href="#background">Background</a></li>
<li><a href="#light-field-sampling">Light Field Sampling</a></li>
<li><a href="#classical-estimation-paradigm">Classical Estimation Paradigm</a>
<ul>
<li><a href="#simple-edge-model">Simple Edge Model</a></li>
</ul></li>
</ul></li>
</ul>
</nav>
    </div>
  

  
    
    
      <div>
        
          <a href="https://twitter.com/share?ref_src=twsrc%5Etfw" class="twitter-share-button" data-show-count="false">Tweet</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
        

        
          <iframe src="https://www.facebook.com/plugins/share_button.php?href=https%3A%2F%2Fdevelopers.facebook.com%2Fdocs%2Fplugins%2F&layout=button&size=small&mobile_iframe=true&width=61&height=20&appId" width="61" height="20" style="border:none;overflow:hidden" scrolling="no" frameborder="0" allowTransparency="true" allow="encrypted-media"></iframe>
        

        

        
          <a data-pocket-label="pocket" data-pocket-count="none" class="pocket-btn" data-lang="en"></a>
          <script type="text/javascript">!function(d,i){if(!d.getElementById(i)){var j=d.createElement("script");j.id=i;j.src="https://widgets.getpocket.com/v1/j/btn.js?v=1";var w=d.getElementById(i);d.body.appendChild(j);}}(document,"pocket-btn-js");</script>
        

      </div>
    
  

        </aside>
      </div>

      <div id="footer" class="pt-2 pb-3 bg-white text-center">
        

  <span class="text-small text-gray">
    

    Powered by the
    <a href="https://github.com/qqhann/hugo-primer" class="link-gray-dark">Hugo-Primer</a> theme for
    <a href="https://gohugo.io" class="link-gray-dark">Hugo</a>.
  </span>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: { equationNumbers: { autoNumber: "AMS" } }
    });
</script>
      </div>
    </div>


    
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/x-mathjax-config">MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });</script>
  </body>
</html>
